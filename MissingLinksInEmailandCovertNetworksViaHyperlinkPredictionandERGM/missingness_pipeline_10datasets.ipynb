{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cdff61",
   "metadata": {},
   "source": [
    "# EPJDS computations: missing links via hyperlink prediction (10 datasets)\n",
    "\n",
    "This notebook runs the leakage-free hyperlink-prediction experiments and writes **all outputs** into the manuscript folders:\n",
    "\n",
    "- Figures → `../figures/`\n",
    "- Tables (LaTeX-ready) → `../tables/`\n",
    "- Raw results / edgelists → `../data/`\n",
    "\n",
    "**Dataset order used everywhere (tables/plots):**\n",
    "1) Epstein (public email, Jmail-derived)  \n",
    "2) Enron (public email, SNAP) — core-100  \n",
    "3) EU-core (public email, SNAP)  \n",
    "4) uni_email / URV (public email, Arenas)  \n",
    "5–10) six covert-network proxies\n",
    "\n",
    "All graphs are analyzed as **simple, undirected, unweighted** networks for comparability:\n",
    "email graphs are symmetrized and time-aggregated.\n",
    "\n",
    "**Methods computed**\n",
    "Common Neighbors; Jaccard; Adamic–Adar; Resource Allocation; Preferential Attachment; Katz (truncated); Personalized PageRank (RWR); node2vec (optional); CHESHIRE (RF); Null-Tie; Matrix Completion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f2e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import gzip\n",
    "import zipfile\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, log_loss, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: node2vec baseline (gensim)\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    _HAVE_GENSIM = True\n",
    "except Exception:\n",
    "    Word2Vec = None\n",
    "    _HAVE_GENSIM = False\n",
    "\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e357d09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/moses/WorkPlaces/Python Projects 2/0 0 EFrelated Missingness\n",
      "Epstein CSV: /Users/moses/WorkPlaces/Python Projects 2/0 0 EFrelated Missingness/data/Epstein_emails_from_jmail_ALL_chrs.csv\n",
      "gensim available: True\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "RNG = np.random.default_rng(42)\n",
    "\n",
    "# If you run from notebooks/, ROOT should be the project root; otherwise adjust.\n",
    "CWD = Path.cwd()\n",
    "ROOT = CWD.parent if (CWD.name.lower() in {\"notebooks\", \"notebook\"}) else CWD\n",
    "\n",
    "FIG_DIR = ROOT / \"figures\"\n",
    "TAB_DIR = ROOT / \"tables\"\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "RAW_DIR  = DATA_DIR / \"raw\"\n",
    "EDGE_DIR = DATA_DIR / \"edgelists\"\n",
    "\n",
    "for d in [FIG_DIR, TAB_DIR, DATA_DIR, RAW_DIR, EDGE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Email datasets (downloaded if missing) ---\n",
    "# SNAP\n",
    "ENRON_URL   = \"https://snap.stanford.edu/data/email-Enron.txt.gz\"\n",
    "EUCORE_URL  = \"https://snap.stanford.edu/data/email-Eu-core.txt.gz\"\n",
    "# Netzschleuder (CSV zip) for URV email\n",
    "URV_URL     = \"https://networks.skewed.de/net/uni_email/files/uni_email.csv.zip\"\n",
    "\n",
    "# Epstein CSV (you provide)\n",
    "# We try several common locations; adjust if needed.\n",
    "EPSTEIN_CSV_CANDIDATES = [\n",
    "    ROOT / \"Epstein_emails_from_jmail_ALL_chrs.csv\",\n",
    "    ROOT.parent / \"Epstein_emails_from_jmail_ALL_chrs.csv\",\n",
    "    ROOT / \"data\" / \"Epstein_emails_from_jmail_ALL_chrs.csv\",\n",
    "    ROOT.parent / \"data\" / \"Epstein_emails_from_jmail_ALL_chrs.csv\",\n",
    "]\n",
    "EPSTEIN_CSV = next((p for p in EPSTEIN_CSV_CANDIDATES if p.exists()), EPSTEIN_CSV_CANDIDATES[0])\n",
    "\n",
    "# --- Experimental parameters ---\n",
    "MISSING_RATES = [0.1, 0.3, 0.5]\n",
    "NUM_TRIALS = 5\n",
    "\n",
    "# To keep runtime bounded on larger graphs, we subsample training/testing hyperedges\n",
    "MAX_TRAIN_HYPEREDGES = 5000\n",
    "MAX_TEST_HYPEREDGES  = 5000\n",
    "\n",
    "# Hyperedge definition: dyads + triangles (recommended for scalability)\n",
    "USE_TRIANGLES = True\n",
    "\n",
    "# node2vec baseline is optional (requires gensim). Also cap by size.\n",
    "ENABLE_NODE2VEC = True\n",
    "NODE2VEC_MAX_N = 5000\n",
    "\n",
    "# Enron: induce a core subgraph for tractability\n",
    "ENRON_CORE_K = 100\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"Epstein CSV:\", EPSTEIN_CSV)\n",
    "print(\"gensim available:\", _HAVE_GENSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b93d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- Utilities ----------\n",
    "\n",
    "def _download(url: str, dest: Path) -> Path:\n",
    "    # Download url -> dest if dest does not exist.\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dest.exists() and dest.stat().st_size > 0:\n",
    "        return dest\n",
    "    print(f\"Downloading: {url} -> {dest}\")\n",
    "    urllib.request.urlretrieve(url, dest)\n",
    "    return dest\n",
    "\n",
    "def _read_txt_gz_edges(path: Path, sep=None, comment_prefix=\"#\"):\n",
    "    # Read SNAP-style .txt.gz edge list into a list of (u,v) strings.\n",
    "    edges = []\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or (comment_prefix and line.startswith(comment_prefix)):\n",
    "                continue\n",
    "            parts = line.split() if sep is None else line.split(sep)\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            u, v = parts[0], parts[1]\n",
    "            if u != v:\n",
    "                edges.append((str(u), str(v)))\n",
    "    return edges\n",
    "\n",
    "def load_snap_txt_gz(url: str, name: str) -> Path:\n",
    "    # Download SNAP txt.gz into data/raw/ and return local path.\n",
    "    dest = RAW_DIR / f\"{name}.txt.gz\"\n",
    "    return _download(url, dest)\n",
    "\n",
    "def load_undirected_from_edges(edges):\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    return G\n",
    "\n",
    "def load_email_snap_graph(url: str, name: str, symmetrize: bool = True):\n",
    "    path = load_snap_txt_gz(url, name)\n",
    "    edges = _read_txt_gz_edges(path)\n",
    "    # SNAP files list directed pairs; for our experiments we symmetrize to undirected\n",
    "    if symmetrize:\n",
    "        return load_undirected_from_edges(edges)\n",
    "    else:\n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(edges)\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "        return G\n",
    "\n",
    "def load_csv_zip(url: str, name: str) -> pd.DataFrame:\n",
    "    # Download a zip containing a CSV and return the dataframe.\n",
    "    dest = RAW_DIR / f\"{name}.csv.zip\"\n",
    "    _download(url, dest)\n",
    "    with zipfile.ZipFile(dest, \"r\") as zf:\n",
    "        members = [m for m in zf.namelist() if m.lower().endswith(\".csv\")]\n",
    "        if not members:\n",
    "            raise ValueError(f\"No CSV found inside {dest}\")\n",
    "        with zf.open(members[0]) as f:\n",
    "            data = f.read()\n",
    "    df = pd.read_csv(io.BytesIO(data))\n",
    "    return df\n",
    "\n",
    "def load_uni_email_urv(url: str = URV_URL) -> nx.Graph:\n",
    "    df = load_csv_zip(url, \"uni_email_urv\")\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(\"URV email CSV must have at least two columns.\")\n",
    "    ucol, vcol = df.columns[0], df.columns[1]\n",
    "    edges = df[[ucol, vcol]].dropna()\n",
    "    edges[ucol] = edges[ucol].astype(str)\n",
    "    edges[vcol] = edges[vcol].astype(str)\n",
    "    return load_undirected_from_edges(list(edges.itertuples(index=False, name=None)))\n",
    "\n",
    "def load_edgelist_csv(path: Path, undirected=True, u=\"u\", v=\"v\"):\n",
    "    df = pd.read_csv(path)\n",
    "    if u not in df.columns or v not in df.columns:\n",
    "        u, v = df.columns[:2]\n",
    "    edges = df[[u, v]].dropna()\n",
    "    edges[u] = edges[u].astype(str)\n",
    "    edges[v] = edges[v].astype(str)\n",
    "    if undirected:\n",
    "        a = edges[[u, v]].min(axis=1)\n",
    "        b = edges[[u, v]].max(axis=1)\n",
    "        edges = pd.DataFrame({\"u\": a, \"v\": b})\n",
    "        edges = edges[edges[\"u\"] != edges[\"v\"]]\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edges.itertuples(index=False, name=None))\n",
    "    else:\n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(edges.itertuples(index=False, name=None))\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    return G\n",
    "\n",
    "def graph_stats(G: nx.Graph):\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    dens = (2*m)/(n*(n-1)) if n > 1 else 0.0\n",
    "    tri = int(sum(nx.triangles(G).values()) // 3) if isinstance(G, nx.Graph) else 0\n",
    "    return {\"n\": n, \"m\": m, \"density\": dens, \"triangles\": tri}\n",
    "\n",
    "def build_epstein_graph(csv_path: Path, year_min=2007, year_max=2025, undirected=True):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=[\"SENDER\", \"RECIPIENT\", \"DATE\"])\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"DATE\"])\n",
    "    df = df[(df[\"DATE\"].dt.year >= year_min) & (df[\"DATE\"].dt.year <= year_max)]\n",
    "    df[\"SENDER\"] = df[\"SENDER\"].astype(str)\n",
    "    df[\"RECIPIENT\"] = df[\"RECIPIENT\"].astype(str)\n",
    "\n",
    "    if undirected:\n",
    "        a = df[[\"SENDER\", \"RECIPIENT\"]].min(axis=1)\n",
    "        b = df[[\"SENDER\", \"RECIPIENT\"]].max(axis=1)\n",
    "        pairs = pd.DataFrame({\"u\": a, \"v\": b})\n",
    "        pairs = pairs[pairs[\"u\"] != pairs[\"v\"]]\n",
    "        w = pairs.value_counts().reset_index(name=\"weight\")\n",
    "        G = nx.Graph()\n",
    "        for _, r in w.iterrows():\n",
    "            G.add_edge(r[\"u\"], r[\"v\"], weight=int(r[\"weight\"]))\n",
    "    else:\n",
    "        w = df.groupby([\"SENDER\", \"RECIPIENT\"]).size().reset_index(name=\"weight\")\n",
    "        G = nx.DiGraph()\n",
    "        for _, r in w.iterrows():\n",
    "            if r[\"SENDER\"] != r[\"RECIPIENT\"]:\n",
    "                G.add_edge(r[\"SENDER\"], r[\"RECIPIENT\"], weight=int(r[\"weight\"]))\n",
    "    return G, df\n",
    "\n",
    "def induce_core_by_activity(G: nx.Graph, k=100):\n",
    "    act = {}\n",
    "    if any(\"weight\" in d for _, _, d in G.edges(data=True)):\n",
    "        for n in G.nodes():\n",
    "            act[n] = sum(d.get(\"weight\", 1.0) for _, _, d in G.edges(n, data=True))\n",
    "    else:\n",
    "        act = dict(G.degree())\n",
    "    top = sorted(act.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    nodes = [n for n, _ in top]\n",
    "    return G.subgraph(nodes).copy()\n",
    "\n",
    "# ---- Hyperedges: dyads + triangles (recommended) ----\n",
    "def triangle_hyperedges(G: nx.Graph):\n",
    "    nodes = list(G.nodes())\n",
    "    order = {n: i for i, n in enumerate(nodes)}\n",
    "    neigh = {u: set(G.neighbors(u)) for u in nodes}\n",
    "    tri = set()\n",
    "    for u in nodes:\n",
    "        Nu = neigh[u]\n",
    "        for v in Nu:\n",
    "            if order[v] <= order[u]:\n",
    "                continue\n",
    "            common = Nu.intersection(neigh.get(v, set()))\n",
    "            for w in common:\n",
    "                if order[w] <= order[v]:\n",
    "                    continue\n",
    "                tri.add(frozenset((u, v, w)))\n",
    "    return list(tri)\n",
    "\n",
    "def build_hyperedges(G: nx.Graph):\n",
    "    dyads = [frozenset(e) for e in G.edges()]\n",
    "    tris = triangle_hyperedges(G) if USE_TRIANGLES else []\n",
    "    E = list(dict.fromkeys(dyads + tris))\n",
    "    return E, dyads, tris\n",
    "\n",
    "def sample_missing_hyperedges(E, missing_rate, rng):\n",
    "    n = len(E)\n",
    "    k = max(1, int(round(missing_rate * n)))\n",
    "    idx = rng.choice(n, size=k, replace=False)\n",
    "    miss = [E[i] for i in idx]\n",
    "    obs = [E[i] for i in range(n) if i not in set(idx)]\n",
    "    return obs, miss\n",
    "\n",
    "def build_G_obs_from_missing(G: nx.Graph, E_miss):\n",
    "    Gobs = G.copy()\n",
    "    for S in E_miss:\n",
    "        nodes = list(S)\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(i + 1, len(nodes)):\n",
    "                if Gobs.has_edge(nodes[i], nodes[j]):\n",
    "                    Gobs.remove_edge(nodes[i], nodes[j])\n",
    "    return Gobs\n",
    "\n",
    "def hyperdegree(E_obs, nodes):\n",
    "    hd = {n: 0 for n in nodes}\n",
    "    for S in E_obs:\n",
    "        for n in S:\n",
    "            hd[n] = hd.get(n, 0) + 1\n",
    "    return hd\n",
    "\n",
    "def induced_density(G, S):\n",
    "    nodes = list(S)\n",
    "    if len(nodes) <= 1:\n",
    "        return 0.0\n",
    "    H = G.subgraph(nodes)\n",
    "    m = H.number_of_edges()\n",
    "    n = H.number_of_nodes()\n",
    "    return (2 * m) / (n * (n - 1)) if n > 1 else 0.0\n",
    "\n",
    "def avg_clustering_on_nodes(G, S):\n",
    "    nodes = list(S)\n",
    "    if not nodes:\n",
    "        return 0.0\n",
    "    cl = nx.clustering(G, nodes=nodes)\n",
    "    return float(np.mean(list(cl.values()))) if cl else 0.0\n",
    "\n",
    "def avg_pairwise_metric(S, metric_func):\n",
    "    nodes = list(S)\n",
    "    if len(nodes) < 2:\n",
    "        return 0.0\n",
    "    s = 0.0\n",
    "    cnt = 0\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i + 1, len(nodes)):\n",
    "            s += float(metric_func(nodes[i], nodes[j]))\n",
    "            cnt += 1\n",
    "    return s / cnt if cnt else 0.0\n",
    "\n",
    "def hyperlink_features(G_obs, E_obs, S):\n",
    "    nodes = list(S)\n",
    "    deg = dict(G_obs.degree())\n",
    "    hd = hyperdegree(E_obs, G_obs.nodes())\n",
    "    cn = cn_score_factory(G_obs)\n",
    "    feats = {\n",
    "        \"size\": len(nodes),\n",
    "        \"avg_degree\": float(np.mean([deg.get(n, 0) for n in nodes])) if nodes else 0.0,\n",
    "        \"induced_density\": induced_density(G_obs, S),\n",
    "        \"avg_clustering\": avg_clustering_on_nodes(G_obs, S),\n",
    "        \"avg_pair_cn\": avg_pairwise_metric(S, cn),\n",
    "        \"avg_hyperdegree\": float(np.mean([hd.get(n, 0) for n in nodes])) if nodes else 0.0,\n",
    "    }\n",
    "    return feats\n",
    "\n",
    "def random_negative_hyperedges(all_nodes, sizes, existing_set, rng, max_tries=50000):\n",
    "    all_nodes = list(all_nodes)\n",
    "    sizes = list(sizes)\n",
    "    neg = []\n",
    "    tries = 0\n",
    "    while len(neg) < len(sizes) and tries < max_tries:\n",
    "        s = int(sizes[len(neg)])\n",
    "        cand = frozenset(rng.choice(all_nodes, size=s, replace=False))\n",
    "        tries += 1\n",
    "        if cand in existing_set:\n",
    "            continue\n",
    "        neg.append(cand)\n",
    "    return neg[: len(sizes)]\n",
    "\n",
    "# ---- Pairwise baseline score factories ----\n",
    "def cn_score_factory(G):\n",
    "    neigh = {n: set(G.neighbors(n)) for n in G.nodes()}\n",
    "    def cn(u, v):\n",
    "        return len(neigh.get(u, set()).intersection(neigh.get(v, set())))\n",
    "    return cn\n",
    "\n",
    "def jaccard_score_factory(G):\n",
    "    neigh = {n: set(G.neighbors(n)) for n in G.nodes()}\n",
    "    def jac(u, v):\n",
    "        Nu, Nv = neigh.get(u, set()), neigh.get(v, set())\n",
    "        inter = len(Nu.intersection(Nv))\n",
    "        uni = len(Nu.union(Nv))\n",
    "        return inter / uni if uni else 0.0\n",
    "    return jac\n",
    "\n",
    "def aa_score_factory(G):\n",
    "    neigh = {n: set(G.neighbors(n)) for n in G.nodes()}\n",
    "    deg = dict(G.degree())\n",
    "    def aa(u, v):\n",
    "        inter = neigh.get(u, set()).intersection(neigh.get(v, set()))\n",
    "        s = 0.0\n",
    "        for w in inter:\n",
    "            dw = deg.get(w, 0)\n",
    "            if dw > 1:\n",
    "                s += 1.0 / np.log(dw)\n",
    "        return s\n",
    "    return aa\n",
    "\n",
    "def ra_score_factory(G):\n",
    "    neigh = {n: set(G.neighbors(n)) for n in G.nodes()}\n",
    "    deg = dict(G.degree())\n",
    "    def ra(u, v):\n",
    "        inter = neigh.get(u, set()).intersection(neigh.get(v, set()))\n",
    "        s = 0.0\n",
    "        for w in inter:\n",
    "            dw = deg.get(w, 0)\n",
    "            if dw > 0:\n",
    "                s += 1.0 / dw\n",
    "        return s\n",
    "    return ra\n",
    "\n",
    "def pa_score_factory(G):\n",
    "    deg = dict(G.degree())\n",
    "    def pa(u, v):\n",
    "        return float(deg.get(u, 0) * deg.get(v, 0))\n",
    "    return pa\n",
    "\n",
    "def katz_pair_matrix(G: nx.Graph, beta=0.005, max_l=3):\n",
    "    nodes = list(G.nodes())\n",
    "    idx = {n: i for i, n in enumerate(nodes)}\n",
    "    if not nodes:\n",
    "        return nodes, idx, None\n",
    "    A = nx.to_scipy_sparse_array(G, nodelist=nodes, dtype=float, format=\"csr\")\n",
    "    K = beta * A\n",
    "    Apow = A\n",
    "    for l in range(2, max_l + 1):\n",
    "        Apow = Apow @ A\n",
    "        K = K + (beta ** l) * Apow\n",
    "    return nodes, idx, K\n",
    "\n",
    "def ppr_score_factory(G: nx.Graph, alpha=0.85, max_iter=100, tol=1e-6):\n",
    "    cache = {}\n",
    "    nodes = list(G.nodes())\n",
    "    base = {n: 0.0 for n in nodes}\n",
    "    def ppr(u, v):\n",
    "        if u not in cache:\n",
    "            pers = dict(base)\n",
    "            if u in pers:\n",
    "                pers[u] = 1.0\n",
    "            cache[u] = nx.pagerank(G, alpha=alpha, personalization=pers, max_iter=max_iter, tol=tol)\n",
    "        return float(cache[u].get(v, 0.0))\n",
    "    return ppr\n",
    "\n",
    "def node2vec_embeddings(G: nx.Graph, dimensions=64, walk_length=20, num_walks=10, window=10, min_count=1, workers=1, seed=42):\n",
    "    if not _HAVE_GENSIM:\n",
    "        raise RuntimeError(\"gensim not available (install gensim to use node2vec baseline).\")\n",
    "    rng = np.random.default_rng(seed)\n",
    "    nodes = list(G.nodes())\n",
    "    neigh = {u: list(G.neighbors(u)) for u in nodes}\n",
    "    walks = []\n",
    "    for _ in range(num_walks):\n",
    "        rng.shuffle(nodes)\n",
    "        for start in nodes:\n",
    "            walk = [str(start)]\n",
    "            cur = start\n",
    "            for _t in range(walk_length - 1):\n",
    "                nbrs = neigh.get(cur, [])\n",
    "                if not nbrs:\n",
    "                    break\n",
    "                cur = rng.choice(nbrs)\n",
    "                walk.append(str(cur))\n",
    "            walks.append(walk)\n",
    "    model = Word2Vec(\n",
    "        sentences=walks,\n",
    "        vector_size=dimensions,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=workers,\n",
    "        seed=seed,\n",
    "        epochs=5\n",
    "    )\n",
    "    emb = {n: model.wv[str(n)] for n in G.nodes() if str(n) in model.wv}\n",
    "    return emb\n",
    "\n",
    "def embedding_score_factory(emb: dict):\n",
    "    def score(u, v):\n",
    "        if u not in emb or v not in emb:\n",
    "            return 0.0\n",
    "        return float(np.dot(emb[u], emb[v]))\n",
    "    return score\n",
    "\n",
    "def matrix_completion_pair_scores(G, rank=10):\n",
    "    nodes = list(G.nodes())\n",
    "    n = len(nodes)\n",
    "    if n == 0:\n",
    "        return lambda u, v: 0.0\n",
    "    A = nx.to_numpy_array(G, nodelist=nodes, dtype=float)\n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    r = min(rank, len(s))\n",
    "    Ahat = (U[:, :r] * s[:r]) @ Vt[:r, :]\n",
    "    idx = {n: i for i, n in enumerate(nodes)}\n",
    "    def mc(u, v):\n",
    "        iu, iv = idx.get(u, None), idx.get(v, None)\n",
    "        if iu is None or iv is None:\n",
    "            return 0.0\n",
    "        return float(Ahat[iu, iv])\n",
    "    return mc\n",
    "\n",
    "def evaluate_trial(G, label, missing_rate, trial, rng):\n",
    "    E_all, dyads, tris = build_hyperedges(G)\n",
    "    E_obs, E_miss = sample_missing_hyperedges(E_all, missing_rate, rng)\n",
    "\n",
    "    if len(E_miss) > MAX_TEST_HYPEREDGES:\n",
    "        idx = rng.choice(len(E_miss), size=MAX_TEST_HYPEREDGES, replace=False)\n",
    "        E_miss = [E_miss[i] for i in idx]\n",
    "\n",
    "    G_obs = build_G_obs_from_missing(G, E_miss)\n",
    "    existing_set = set(E_all)\n",
    "\n",
    "    if len(E_obs) > MAX_TRAIN_HYPEREDGES:\n",
    "        idx = rng.choice(len(E_obs), size=MAX_TRAIN_HYPEREDGES, replace=False)\n",
    "        E_obs_train = [E_obs[i] for i in idx]\n",
    "    else:\n",
    "        E_obs_train = E_obs\n",
    "\n",
    "    train_sizes = [len(S) for S in E_obs_train]\n",
    "    neg_train = random_negative_hyperedges(G.nodes(), train_sizes, existing_set, rng)\n",
    "    y = np.array([1] * len(E_obs_train) + [0] * len(neg_train))\n",
    "    X = pd.DataFrame([hyperlink_features(G_obs, E_obs_train, S) for S in (E_obs_train + neg_train)])\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=trial, stratify=y)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=300, random_state=trial, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    test_sizes = [len(S) for S in E_miss]\n",
    "    neg_test = random_negative_hyperedges(G.nodes(), test_sizes, existing_set, rng)\n",
    "    X_test = pd.DataFrame([hyperlink_features(G_obs, E_obs_train, S) for S in (E_miss + neg_test)])\n",
    "    y_test = np.array([1] * len(E_miss) + [0] * len(neg_test))\n",
    "\n",
    "    p_cheshire = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    cn = cn_score_factory(G_obs)\n",
    "    jac = jaccard_score_factory(G_obs)\n",
    "    aa = aa_score_factory(G_obs)\n",
    "    ra = ra_score_factory(G_obs)\n",
    "    pa = pa_score_factory(G_obs)\n",
    "\n",
    "    _, idx_k, K = katz_pair_matrix(G_obs, beta=0.005, max_l=3)\n",
    "    def katz(u, v):\n",
    "        iu, iv = idx_k.get(u, None), idx_k.get(v, None)\n",
    "        if iu is None or iv is None or K is None:\n",
    "            return 0.0\n",
    "        return float(K[iu, iv])\n",
    "\n",
    "    ppr = ppr_score_factory(G_obs, alpha=0.85)\n",
    "    mc = matrix_completion_pair_scores(G_obs, rank=10)\n",
    "\n",
    "    have_n2v = bool(ENABLE_NODE2VEC and _HAVE_GENSIM and G_obs.number_of_nodes() <= NODE2VEC_MAX_N)\n",
    "    if have_n2v:\n",
    "        emb = node2vec_embeddings(G_obs, dimensions=64, walk_length=20, num_walks=10, window=10, workers=1, seed=42)\n",
    "        n2v = embedding_score_factory(emb)\n",
    "\n",
    "    def score_hyperedge(S, pair_score):\n",
    "        return avg_pairwise_metric(S, pair_score)\n",
    "\n",
    "    test_hyperedges = E_miss + neg_test\n",
    "    scores = OrderedDict()\n",
    "    scores[\"Null-Tie\"] = np.full_like(y_test, 0.5, dtype=float)\n",
    "    scores[\"Common Neighbors\"] = np.array([score_hyperedge(S, cn) for S in test_hyperedges], dtype=float)\n",
    "    scores[\"Jaccard\"] = np.array([score_hyperedge(S, jac) for S in test_hyperedges], dtype=float)\n",
    "    scores[\"Adamic–Adar\"] = np.array([score_hyperedge(S, aa) for S in test_hyperedges], dtype=float)\n",
    "    scores[\"Resource Allocation\"] = np.array([score_hyperedge(S, ra) for S in test_hyperedges], dtype=float)\n",
    "    scores[\"Preferential Attachment\"] = np.array([score_hyperedge(S, pa) for S in test_hyperedges], dtype=float)\n",
    "    scores[\"Katz\"] = np.array([score_hyperedge(S, katz) for S in test_hyperedges], dtype=float)\n",
    "    scores[\"Personalized PageRank\"] = np.array([score_hyperedge(S, ppr) for S in test_hyperedges], dtype=float)\n",
    "    if have_n2v:\n",
    "        scores[\"node2vec\"] = np.array([score_hyperedge(S, n2v) for S in test_hyperedges], dtype=float)\n",
    "    scores[\"Matrix Completion\"] = np.array([score_hyperedge(S, mc) for S in test_hyperedges], dtype=float)\n",
    "    scores[\"CHESHIRE\"] = p_cheshire\n",
    "\n",
    "    for k, v in scores.items():\n",
    "        if k in {\"Null-Tie\", \"CHESHIRE\"}:\n",
    "            continue\n",
    "        mn, mx = float(np.min(v)), float(np.max(v))\n",
    "        denom = (mx - mn) if mx > mn else 1.0\n",
    "        scores[k] = (v - mn) / denom\n",
    "\n",
    "    rows = []\n",
    "    for method, pred in scores.items():\n",
    "        auc = roc_auc_score(y_test, pred) if len(np.unique(y_test)) > 1 else 0.5\n",
    "        yhat = (pred >= 0.5).astype(int)\n",
    "        rows.append({\n",
    "            \"dataset\": label,\n",
    "            \"missing_rate\": float(missing_rate),\n",
    "            \"trial\": int(trial),\n",
    "            \"method\": method,\n",
    "            \"ROC-AUC\": float(auc),\n",
    "            \"F1\": float(f1_score(y_test, yhat)),\n",
    "            \"Accuracy\": float(accuracy_score(y_test, yhat)),\n",
    "            \"Log-loss\": float(log_loss(y_test, np.clip(pred, 1e-9, 1 - 1e-9))),\n",
    "            \"MCC\": float(matthews_corrcoef(y_test, yhat)),\n",
    "            \"n_nodes\": int(G.number_of_nodes()),\n",
    "            \"n_edges\": int(G.number_of_edges()),\n",
    "            \"n_hyperedges\": int(len(E_all)),\n",
    "            \"n_test\": int(len(E_miss))\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def save_edgelist(G: nx.Graph, path: Path):\n",
    "    df = pd.DataFrame(list(G.edges()), columns=[\"u\", \"v\"])\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def _latex_clean(s: str) -> str:\n",
    "    return str(s).replace(\"–\", \"--\").replace(\"—\", \"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860aead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded covert proxy datasets: ['Christmas Eve Bombings 2000 (proxy)', 'Bali Bombing 2002 (proxy)', 'Australian Embassy Bombing 2004 (proxy)', 'Bali Bombing 2005 (proxy)', 'Hamburg Cell 9/11 2001 (proxy)', 'London Gang 2005–2009 (proxy)']\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load covert-network proxy edgelists (6) ----------\n",
    "proxy_files = [\n",
    "    \"christmas_eve_2000.csv\",\n",
    "    \"bali_2002.csv\",\n",
    "    \"australian_embassy_2004.csv\",\n",
    "    \"bali_2005.csv\",\n",
    "    \"hamburg_cell_911.csv\",\n",
    "    \"london_gang.csv\",\n",
    "]\n",
    "\n",
    "proxy_labels = {\n",
    "    \"christmas_eve_2000.csv\": \"Christmas Eve Bombings 2000 (proxy)\",\n",
    "    \"bali_2002.csv\": \"Bali Bombing 2002 (proxy)\",\n",
    "    \"australian_embassy_2004.csv\": \"Australian Embassy Bombing 2004 (proxy)\",\n",
    "    \"bali_2005.csv\": \"Bali Bombing 2005 (proxy)\",\n",
    "    \"hamburg_cell_911.csv\": \"Hamburg Cell 9/11 2001 (proxy)\",\n",
    "    \"london_gang.csv\": \"London Gang 2005–2009 (proxy)\",\n",
    "}\n",
    "\n",
    "proxies = OrderedDict()\n",
    "for fn in proxy_files:\n",
    "    path = EDGE_DIR / fn\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing edgelist: {path}\")\n",
    "    G = load_edgelist_csv(path, undirected=True)\n",
    "    proxies[proxy_labels[fn]] = G\n",
    "\n",
    "print(\"Loaded covert proxy datasets:\", list(proxies.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bfb929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://snap.stanford.edu/data/email-Enron.txt.gz -> /Users/moses/WorkPlaces/Python Projects 2/0 0 EFrelated Missingness/data/raw/email-Enron.txt.gz\n",
      "Downloading: https://snap.stanford.edu/data/email-Eu-core.txt.gz -> /Users/moses/WorkPlaces/Python Projects 2/0 0 EFrelated Missingness/data/raw/email-Eu-core.txt.gz\n",
      "Downloading: https://networks.skewed.de/net/uni_email/files/uni_email.csv.zip -> /Users/moses/WorkPlaces/Python Projects 2/0 0 EFrelated Missingness/data/raw/uni_email_urv.csv.zip\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>n</th>\n",
       "      <th>m</th>\n",
       "      <th>density</th>\n",
       "      <th>triangles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Epstein emails 2007–2025 (full)</td>\n",
       "      <td>426</td>\n",
       "      <td>543</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Enron emails (core-100)</td>\n",
       "      <td>100</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.300404</td>\n",
       "      <td>9050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EU-core emails (SNAP)</td>\n",
       "      <td>986</td>\n",
       "      <td>16064</td>\n",
       "      <td>0.033080</td>\n",
       "      <td>105461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uni_email (URV)</td>\n",
       "      <td>1133</td>\n",
       "      <td>5451</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>5343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Christmas Eve Bombings 2000 (proxy)</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bali Bombing 2002 (proxy)</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Australian Embassy Bombing 2004 (proxy)</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bali Bombing 2005 (proxy)</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hamburg Cell 9/11 2001 (proxy)</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>London Gang 2005–2009 (proxy)</td>\n",
       "      <td>50</td>\n",
       "      <td>85</td>\n",
       "      <td>0.069388</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   dataset     n      m   density  triangles\n",
       "0          Epstein emails 2007–2025 (full)   426    543  0.005998        161\n",
       "1                  Enron emails (core-100)   100   1487  0.300404       9050\n",
       "2                    EU-core emails (SNAP)   986  16064  0.033080     105461\n",
       "3                          uni_email (URV)  1133   5451  0.008500       5343\n",
       "4      Christmas Eve Bombings 2000 (proxy)    14     16  0.175824          5\n",
       "5                Bali Bombing 2002 (proxy)    15     24  0.228571         22\n",
       "6  Australian Embassy Bombing 2004 (proxy)    10     15  0.333333          8\n",
       "7                Bali Bombing 2005 (proxy)     9     15  0.416667         11\n",
       "8           Hamburg Cell 9/11 2001 (proxy)    12     23  0.348485         23\n",
       "9            London Gang 2005–2009 (proxy)    50     85  0.069388         46"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Load email datasets + build final OrderedDict (10 datasets) ----------\n",
    "\n",
    "# Epstein (Jmail-derived, local CSV)\n",
    "if not EPSTEIN_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Epstein CSV not found at: {EPSTEIN_CSV} (edit EPSTEIN_CSV_CANDIDATES in config)\")\n",
    "G_ep, _df_ep = build_epstein_graph(EPSTEIN_CSV, year_min=2007, year_max=2025, undirected=True)\n",
    "\n",
    "# Enron (SNAP) — build full undirected, then core-100 by activity\n",
    "G_en_full = load_email_snap_graph(ENRON_URL, \"email-Enron\", symmetrize=True)\n",
    "G_en_core = induce_core_by_activity(G_en_full, k=ENRON_CORE_K)\n",
    "\n",
    "# EU-core (SNAP) — undirected (symmetrized)\n",
    "G_eu = load_email_snap_graph(EUCORE_URL, \"email-Eu-core\", symmetrize=True)\n",
    "\n",
    "# URV uni_email (Netzschleuder)\n",
    "G_urv = load_uni_email_urv(URV_URL)\n",
    "\n",
    "# Save edgelists for reproducibility / downstream ERGM\n",
    "save_edgelist(G_ep, EDGE_DIR / \"epstein_emails_2007_2025_full.csv\")\n",
    "save_edgelist(G_en_core, EDGE_DIR / \"enron_emails_core100.csv\")\n",
    "save_edgelist(G_eu, EDGE_DIR / \"email_eu_core.csv\")\n",
    "save_edgelist(G_urv, EDGE_DIR / \"uni_email_urv.csv\")\n",
    "\n",
    "# Final dataset order for ALL results/tables/plots\n",
    "datasets = OrderedDict()\n",
    "datasets[\"Epstein emails 2007–2025 (full)\"] = G_ep\n",
    "datasets[\"Enron emails (core-100)\"] = G_en_core\n",
    "datasets[\"EU-core emails (SNAP)\"] = G_eu\n",
    "datasets[\"uni_email (URV)\"] = G_urv\n",
    "\n",
    "# append covert proxies (keeps their internal order)\n",
    "for k, v in proxies.items():\n",
    "    datasets[k] = v\n",
    "\n",
    "# Print summary stats\n",
    "stats = []\n",
    "for label, G in datasets.items():\n",
    "    s = graph_stats(G)\n",
    "    s[\"dataset\"] = label\n",
    "    stats.append(s)\n",
    "pd.DataFrame(stats)[[\"dataset\",\"n\",\"m\",\"density\",\"triangles\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034169fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: Epstein emails 2007–2025 (full) | mr=0.1 | trial=0 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.1 | trial=1 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.1 | trial=2 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.1 | trial=3 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.1 | trial=4 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.3 | trial=0 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.3 | trial=1 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.3 | trial=2 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.3 | trial=3 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.3 | trial=4 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.5 | trial=0 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.5 | trial=1 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.5 | trial=2 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.5 | trial=3 | rows=11\n",
      "Done: Epstein emails 2007–2025 (full) | mr=0.5 | trial=4 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.1 | trial=0 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.1 | trial=1 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.1 | trial=2 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.1 | trial=3 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.1 | trial=4 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.3 | trial=0 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.3 | trial=1 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.3 | trial=2 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.3 | trial=3 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.3 | trial=4 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.5 | trial=0 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.5 | trial=1 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.5 | trial=2 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.5 | trial=3 | rows=11\n",
      "Done: Enron emails (core-100) | mr=0.5 | trial=4 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.1 | trial=0 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.1 | trial=1 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.1 | trial=2 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.1 | trial=3 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.1 | trial=4 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.3 | trial=0 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.3 | trial=1 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.3 | trial=2 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.3 | trial=3 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.3 | trial=4 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.5 | trial=0 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.5 | trial=1 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.5 | trial=2 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.5 | trial=3 | rows=11\n",
      "Done: EU-core emails (SNAP) | mr=0.5 | trial=4 | rows=11\n",
      "Done: uni_email (URV) | mr=0.1 | trial=0 | rows=11\n",
      "Done: uni_email (URV) | mr=0.1 | trial=1 | rows=11\n",
      "Done: uni_email (URV) | mr=0.1 | trial=2 | rows=11\n",
      "Done: uni_email (URV) | mr=0.1 | trial=3 | rows=11\n",
      "Done: uni_email (URV) | mr=0.1 | trial=4 | rows=11\n",
      "Done: uni_email (URV) | mr=0.3 | trial=0 | rows=11\n",
      "Done: uni_email (URV) | mr=0.3 | trial=1 | rows=11\n",
      "Done: uni_email (URV) | mr=0.3 | trial=2 | rows=11\n",
      "Done: uni_email (URV) | mr=0.3 | trial=3 | rows=11\n",
      "Done: uni_email (URV) | mr=0.3 | trial=4 | rows=11\n",
      "Done: uni_email (URV) | mr=0.5 | trial=0 | rows=11\n",
      "Done: uni_email (URV) | mr=0.5 | trial=1 | rows=11\n",
      "Done: uni_email (URV) | mr=0.5 | trial=2 | rows=11\n",
      "Done: uni_email (URV) | mr=0.5 | trial=3 | rows=11\n",
      "Done: uni_email (URV) | mr=0.5 | trial=4 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.1 | trial=0 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.1 | trial=1 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.1 | trial=2 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.1 | trial=3 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.1 | trial=4 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.3 | trial=0 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.3 | trial=1 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.3 | trial=2 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.3 | trial=3 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.3 | trial=4 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.5 | trial=0 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.5 | trial=1 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.5 | trial=2 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.5 | trial=3 | rows=11\n",
      "Done: Christmas Eve Bombings 2000 (proxy) | mr=0.5 | trial=4 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.1 | trial=0 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.1 | trial=1 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.1 | trial=2 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.1 | trial=3 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.1 | trial=4 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.3 | trial=0 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.3 | trial=1 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.3 | trial=2 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.3 | trial=3 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.3 | trial=4 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.5 | trial=0 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.5 | trial=1 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.5 | trial=2 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.5 | trial=3 | rows=11\n",
      "Done: Bali Bombing 2002 (proxy) | mr=0.5 | trial=4 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.1 | trial=0 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.1 | trial=1 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.1 | trial=2 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.1 | trial=3 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.1 | trial=4 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.3 | trial=0 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.3 | trial=1 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.3 | trial=2 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.3 | trial=3 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.3 | trial=4 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.5 | trial=0 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.5 | trial=1 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.5 | trial=2 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.5 | trial=3 | rows=11\n",
      "Done: Australian Embassy Bombing 2004 (proxy) | mr=0.5 | trial=4 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.1 | trial=0 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.1 | trial=1 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.1 | trial=2 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.1 | trial=3 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.1 | trial=4 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.3 | trial=0 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.3 | trial=1 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.3 | trial=2 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.3 | trial=3 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.3 | trial=4 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.5 | trial=0 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.5 | trial=1 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.5 | trial=2 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.5 | trial=3 | rows=11\n",
      "Done: Bali Bombing 2005 (proxy) | mr=0.5 | trial=4 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.1 | trial=0 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.1 | trial=1 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.1 | trial=2 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.1 | trial=3 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.1 | trial=4 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.3 | trial=0 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.3 | trial=1 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.3 | trial=2 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.3 | trial=3 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.3 | trial=4 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.5 | trial=0 | rows=11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.5 | trial=1 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.5 | trial=2 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.5 | trial=3 | rows=11\n",
      "Done: Hamburg Cell 9/11 2001 (proxy) | mr=0.5 | trial=4 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.1 | trial=0 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.1 | trial=1 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.1 | trial=2 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.1 | trial=3 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.1 | trial=4 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.3 | trial=0 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.3 | trial=1 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.3 | trial=2 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.3 | trial=3 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.3 | trial=4 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.5 | trial=0 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.5 | trial=1 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.5 | trial=2 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.5 | trial=3 | rows=11\n",
      "Done: London Gang 2005–2009 (proxy) | mr=0.5 | trial=4 | rows=11\n",
      "Total rows: 1650 Elapsed (s): 8930.88\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>missing_rate</th>\n",
       "      <th>trial</th>\n",
       "      <th>method</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Log-loss</th>\n",
       "      <th>MCC</th>\n",
       "      <th>n_nodes</th>\n",
       "      <th>n_edges</th>\n",
       "      <th>n_hyperedges</th>\n",
       "      <th>n_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Epstein emails 2007–2025 (full)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>Null-Tie</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>426</td>\n",
       "      <td>543</td>\n",
       "      <td>704</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Epstein emails 2007–2025 (full)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>Common Neighbors</td>\n",
       "      <td>0.526735</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>6.584479</td>\n",
       "      <td>0.306186</td>\n",
       "      <td>426</td>\n",
       "      <td>543</td>\n",
       "      <td>704</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Epstein emails 2007–2025 (full)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>Jaccard</td>\n",
       "      <td>0.431020</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>8.316134</td>\n",
       "      <td>-0.284293</td>\n",
       "      <td>426</td>\n",
       "      <td>543</td>\n",
       "      <td>704</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Epstein emails 2007–2025 (full)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>Adamic–Adar</td>\n",
       "      <td>0.555714</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>6.581107</td>\n",
       "      <td>0.306186</td>\n",
       "      <td>426</td>\n",
       "      <td>543</td>\n",
       "      <td>704</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Epstein emails 2007–2025 (full)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>Resource Allocation</td>\n",
       "      <td>0.553878</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>6.681741</td>\n",
       "      <td>0.277350</td>\n",
       "      <td>426</td>\n",
       "      <td>543</td>\n",
       "      <td>704</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           dataset  missing_rate  trial               method  \\\n",
       "0  Epstein emails 2007–2025 (full)           0.1      0             Null-Tie   \n",
       "1  Epstein emails 2007–2025 (full)           0.1      0     Common Neighbors   \n",
       "2  Epstein emails 2007–2025 (full)           0.1      0              Jaccard   \n",
       "3  Epstein emails 2007–2025 (full)           0.1      0          Adamic–Adar   \n",
       "4  Epstein emails 2007–2025 (full)           0.1      0  Resource Allocation   \n",
       "\n",
       "    ROC-AUC        F1  Accuracy  Log-loss       MCC  n_nodes  n_edges  \\\n",
       "0  0.500000  0.666667  0.500000  0.693147  0.000000      426      543   \n",
       "1  0.526735  0.292683  0.585714  6.584479  0.306186      426      543   \n",
       "2  0.431020  0.045977  0.407143  8.316134 -0.284293      426      543   \n",
       "3  0.555714  0.292683  0.585714  6.581107  0.306186      426      543   \n",
       "4  0.553878  0.250000  0.571429  6.681741  0.277350      426      543   \n",
       "\n",
       "   n_hyperedges  n_test  \n",
       "0           704      70  \n",
       "1           704      70  \n",
       "2           704      70  \n",
       "3           704      70  \n",
       "4           704      70  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Run missingness experiments ----------\n",
    "all_rows = []\n",
    "t0 = time.time()\n",
    "\n",
    "for label, G in datasets.items():\n",
    "    for mr in MISSING_RATES:\n",
    "        for t in range(NUM_TRIALS):\n",
    "            df_trial = evaluate_trial(G, label, mr, t, RNG)\n",
    "            all_rows.append(df_trial)\n",
    "            print(f\"Done: {label} | mr={mr} | trial={t} | rows={len(df_trial)}\")\n",
    "\n",
    "results = pd.concat(all_rows, ignore_index=True)\n",
    "results.to_csv(DATA_DIR / \"all_results_raw_10datasets.csv\", index=False)\n",
    "\n",
    "print(\"Total rows:\", results.shape[0], \"Elapsed (s):\", round(time.time() - t0, 2))\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60bb7659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/moses/WorkPlaces/Python Projects 2/0 0 EFrelated Missingness/tables/network_stats.tex\n",
      "Wrote: /Users/moses/WorkPlaces/Python Projects 2/0 0 EFrelated Missingness/tables/overall_summary_table.tex\n",
      "Wrote: /Users/moses/WorkPlaces/Python Projects 2/0 0 EFrelated Missingness/tables/rocauc_by_network_table.tex\n",
      "Wrote: /Users/moses/WorkPlaces/Python Projects 2/0 0 EFrelated Missingness/tables/auc_by_missing_rate_table.tex\n",
      "Wrote: /Users/moses/WorkPlaces/Python Projects 2/0 0 EFrelated Missingness/tables/detailed_results_longtable.tex\n"
     ]
    }
   ],
   "source": [
    "# ---------- Summaries + LaTeX tables (written to ../tables/) ----------\n",
    "METHOD_ORDER = [\n",
    "    \"CHESHIRE\",\n",
    "    \"Matrix Completion\",\n",
    "    \"node2vec\",\n",
    "    \"Personalized PageRank\",\n",
    "    \"Katz\",\n",
    "    \"Preferential Attachment\",\n",
    "    \"Resource Allocation\",\n",
    "    \"Adamic–Adar\",\n",
    "    \"Jaccard\",\n",
    "    \"Common Neighbors\",\n",
    "    \"Null-Tie\",\n",
    "]\n",
    "\n",
    "# enforce dataset order\n",
    "dataset_order = list(datasets.keys())\n",
    "results[\"dataset\"] = pd.Categorical(results[\"dataset\"], categories=dataset_order, ordered=True)\n",
    "results[\"method\"] = pd.Categorical(results[\"method\"], categories=METHOD_ORDER, ordered=True)\n",
    "\n",
    "def write_table(tex_path: Path, body: str):\n",
    "    tex_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tex_path.write_text(body, encoding=\"utf-8\")\n",
    "    print(\"Wrote:\", tex_path)\n",
    "\n",
    "def latex_table(df: pd.DataFrame, caption: str, label: str, *, longtable=False, fontsize=\"\\small\"):\n",
    "    df2 = df.copy()\n",
    "    # clean dashes for LaTeX\n",
    "    if \"dataset\" in df2.columns:\n",
    "        df2[\"dataset\"] = df2[\"dataset\"].astype(str).map(_latex_clean)\n",
    "    if \"method\" in df2.columns:\n",
    "        df2[\"method\"] = df2[\"method\"].astype(str).map(_latex_clean)\n",
    "\n",
    "    latex = df2.to_latex(index=False, escape=False, longtable=longtable)\n",
    "    if longtable:\n",
    "        # longtable already has its own environment\n",
    "        out = f\"\"\"{fontsize}\n",
    "{latex}\n",
    "\"\"\"\n",
    "    else:\n",
    "        out = f\"\"\"\\begin{{table}}[t]\n",
    "{fontsize}\n",
    "\\centering\n",
    "\\caption{{{caption}}}\n",
    "\\label{{{label}}}\n",
    "{latex}\n",
    "\\end{{table}}\n",
    "\"\"\"\n",
    "    return out\n",
    "\n",
    "# Network stats\n",
    "net_rows = []\n",
    "for label, G in datasets.items():\n",
    "    s = graph_stats(G)\n",
    "    net_rows.append({\n",
    "        \"dataset\": label,\n",
    "        \"nodes\": s[\"n\"],\n",
    "        \"edges\": s[\"m\"],\n",
    "        \"density\": round(s[\"density\"], 4),\n",
    "        \"triangles\": s[\"triangles\"],\n",
    "    })\n",
    "net_df = pd.DataFrame(net_rows)\n",
    "write_table(TAB_DIR / \"network_stats.tex\",\n",
    "            latex_table(net_df, \"Summary statistics for the 10 interaction graphs (undirected, unweighted).\", \"tab:network_stats\"))\n",
    "\n",
    "# Overall performance summary (mean over datasets, missing rates, trials)\n",
    "overall = (results\n",
    "           .groupby(\"method\", observed=True)[\"ROC-AUC\"]\n",
    "           .agg([\"mean\",\"std\",\"count\"])\n",
    "           .reset_index())\n",
    "overall[\"SE\"] = overall[\"std\"] / np.sqrt(overall[\"count\"])\n",
    "overall = overall.sort_values(\"mean\", ascending=False)\n",
    "overall_disp = overall[[\"method\",\"mean\",\"SE\"]].copy()\n",
    "overall_disp[\"mean\"] = overall_disp[\"mean\"].map(lambda x: f\"{x:.3f}\")\n",
    "overall_disp[\"SE\"] = overall_disp[\"SE\"].map(lambda x: f\"{x:.3f}\")\n",
    "write_table(TAB_DIR / \"overall_summary_table.tex\",\n",
    "            latex_table(overall_disp, \"Overall ROC--AUC (mean $\\pm$ SE) across datasets, missing rates, and trials.\", \"tab:overall_summary\"))\n",
    "\n",
    "# Mean ROC-AUC by dataset and method at rho=0.3\n",
    "rho = 0.3\n",
    "hm = (results[results[\"missing_rate\"] == rho]\n",
    "      .groupby([\"dataset\",\"method\"], observed=True)[\"ROC-AUC\"]\n",
    "      .mean()\n",
    "      .reset_index())\n",
    "hm_piv = hm.pivot(index=\"dataset\", columns=\"method\", values=\"ROC-AUC\").reset_index()\n",
    "# keep method order columns\n",
    "cols = [\"dataset\"] + [m for m in METHOD_ORDER if m in hm_piv.columns]\n",
    "hm_piv = hm_piv[cols]\n",
    "# format\n",
    "for c in cols[1:]:\n",
    "    hm_piv[c] = hm_piv[c].map(lambda x: \"\" if pd.isna(x) else f\"{x:.3f}\")\n",
    "write_table(TAB_DIR / \"rocauc_by_network_table.tex\",\n",
    "            latex_table(hm_piv, f\"Mean ROC--AUC by dataset and method at missing rate $\\\\rho={rho}$.\", \"tab:rocauc_by_network\"))\n",
    "\n",
    "# ROC-AUC vs missing rate (mean over datasets + trials)\n",
    "mr = (results\n",
    "      .groupby([\"method\",\"missing_rate\"], observed=True)[\"ROC-AUC\"]\n",
    "      .mean()\n",
    "      .reset_index())\n",
    "mr_piv = mr.pivot(index=\"method\", columns=\"missing_rate\", values=\"ROC-AUC\").reset_index()\n",
    "# reorder methods\n",
    "mr_piv[\"method\"] = pd.Categorical(mr_piv[\"method\"], categories=METHOD_ORDER, ordered=True)\n",
    "mr_piv = mr_piv.sort_values(\"method\")\n",
    "# format columns\n",
    "mr_piv_cols = [\"method\"] + sorted([c for c in mr_piv.columns if c != \"method\"])\n",
    "mr_piv = mr_piv[mr_piv_cols]\n",
    "for c in mr_piv_cols[1:]:\n",
    "    mr_piv[c] = mr_piv[c].map(lambda x: \"\" if pd.isna(x) else f\"{x:.3f}\")\n",
    "mr_piv = mr_piv.rename(columns={c: f\"$\\\\rho={c}$\" for c in mr_piv_cols[1:]})\n",
    "write_table(TAB_DIR / \"auc_by_missing_rate_table.tex\",\n",
    "            latex_table(mr_piv, \"Mean ROC--AUC as a function of missing rate $\\\\rho$ (averaged across datasets and trials).\", \"tab:auc_by_missing_rate\"))\n",
    "\n",
    "# Detailed longtable (appendix): per dataset, missing rate, method\n",
    "det = (results\n",
    "       .groupby([\"dataset\",\"missing_rate\",\"method\"], observed=True)[\"ROC-AUC\"]\n",
    "       .agg([\"mean\",\"std\",\"count\"])\n",
    "       .reset_index())\n",
    "det[\"SE\"] = det[\"std\"] / np.sqrt(det[\"count\"])\n",
    "det_disp = det[[\"dataset\",\"missing_rate\",\"method\",\"mean\",\"SE\"]].copy()\n",
    "det_disp[\"missing_rate\"] = det_disp[\"missing_rate\"].map(lambda x: f\"{x:.1f}\")\n",
    "det_disp[\"mean\"] = det_disp[\"mean\"].map(lambda x: f\"{x:.3f}\")\n",
    "det_disp[\"SE\"] = det_disp[\"SE\"].map(lambda x: f\"{x:.3f}\")\n",
    "write_table(TAB_DIR / \"detailed_results_longtable.tex\",\n",
    "            latex_table(det_disp, \"Detailed ROC--AUC (mean $\\pm$ SE) by dataset, missing rate, and method.\", \"tab:detailed_results\", longtable=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "596f790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figures to: /Users/moses/WorkPlaces/Python Projects 2/0 0 EFrelated Missingness/figures\n"
     ]
    }
   ],
   "source": [
    "# ---------- Plots (written to ../figures/) ----------\n",
    "\n",
    "# Mean ROC-AUC by missing rate (averaged across datasets & trials)\n",
    "agg_mr = (results\n",
    "          .groupby([\"method\",\"missing_rate\"], observed=True)[\"ROC-AUC\"]\n",
    "          .mean()\n",
    "          .reset_index())\n",
    "agg_mr[\"method\"] = pd.Categorical(agg_mr[\"method\"], categories=METHOD_ORDER, ordered=True)\n",
    "agg_mr = agg_mr.sort_values([\"method\",\"missing_rate\"])\n",
    "\n",
    "plt.figure()\n",
    "for method in METHOD_ORDER:\n",
    "    d = agg_mr[agg_mr[\"method\"] == method]\n",
    "    if d.empty:\n",
    "        continue\n",
    "    plt.plot(d[\"missing_rate\"], d[\"ROC-AUC\"], marker=\"o\", label=method)\n",
    "plt.xlabel(\"Missing rate $\\\\rho$\")\n",
    "plt.ylabel(\"Mean ROC-AUC\")\n",
    "plt.title(\"Performance vs missing rate (mean over datasets & trials)\")\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"performance_vs_missing_rate.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# Heatmap: mean ROC-AUC by dataset and method at rho=0.3\n",
    "rho = 0.3\n",
    "hm = (results[results[\"missing_rate\"] == rho]\n",
    "      .groupby([\"dataset\",\"method\"], observed=True)[\"ROC-AUC\"]\n",
    "      .mean()\n",
    "      .reset_index())\n",
    "hm_piv = hm.pivot(index=\"dataset\", columns=\"method\", values=\"ROC-AUC\")\n",
    "# ensure ordered axes\n",
    "hm_piv = hm_piv.reindex(index=dataset_order, columns=[m for m in METHOD_ORDER if m in hm_piv.columns])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "im = plt.imshow(hm_piv.values, aspect=\"auto\")\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.xticks(range(hm_piv.shape[1]), [m.replace(\"–\",\"-\") for m in hm_piv.columns], rotation=45, ha=\"right\", fontsize=8)\n",
    "plt.yticks(range(hm_piv.shape[0]), [d.replace(\"–\",\"-\") for d in hm_piv.index], fontsize=8)\n",
    "plt.title(f\"Mean ROC-AUC by dataset and method at $\\\\rho={rho}$\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"performance_by_network_heatmap.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved figures to:\", FIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8cb9fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: run_outputs_20260109-0221.zip\n"
     ]
    }
   ],
   "source": [
    "import os, zipfile, time\n",
    "\n",
    "stamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "zip_path = f\"run_outputs_{stamp}.zip\"\n",
    "\n",
    "paths = [\"figures\", \"tables\", \"data\"]\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    for p in paths:\n",
    "        for root, _, files in os.walk(p):\n",
    "            for fn in files:\n",
    "                fp = os.path.join(root, fn)\n",
    "                z.write(fp, fp)\n",
    "\n",
    "print(\"Created:\", zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d646ba",
   "metadata": {},
   "source": [
    "# ---------- Controlled scaling simulation (synthetic graphs) ----------\n",
    "# Produces:\n",
    "# - ../figures/simulation_auc_vs_n.png\n",
    "# - ../figures/simulation_cheshire_fit_time.png\n",
    "# - ../tables/scaling_table.tex\n",
    "\n",
    "def make_sbm(n, k=4, p_in=0.05, p_out=0.005, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sizes = [n // k] * k\n",
    "    sizes[0] += n - sum(sizes)\n",
    "    probs = [[p_out]*k for _ in range(k)]\n",
    "    for i in range(k):\n",
    "        probs[i][i] = p_in\n",
    "    G = nx.stochastic_block_model(sizes, probs, seed=seed)\n",
    "    # simple undirected graph\n",
    "    G = nx.Graph(G)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    # relabel nodes to strings for consistency\n",
    "    return nx.relabel_nodes(G, {u: str(u) for u in G.nodes()})\n",
    "\n",
    "def cheshire_fit_time_and_auc(G, missing_rate=0.3, trial=0, rng=None):\n",
    "    rng = RNG if rng is None else rng\n",
    "    E_all, _, _ = build_hyperedges(G)\n",
    "    E_obs, E_miss = sample_missing_hyperedges(E_all, missing_rate, rng)\n",
    "\n",
    "    if len(E_miss) > MAX_TEST_HYPEREDGES:\n",
    "        idx = rng.choice(len(E_miss), size=MAX_TEST_HYPEREDGES, replace=False)\n",
    "        E_miss = [E_miss[i] for i in idx]\n",
    "\n",
    "    G_obs = build_G_obs_from_missing(G, E_miss)\n",
    "    existing_set = set(E_all)\n",
    "\n",
    "    if len(E_obs) > MAX_TRAIN_HYPEREDGES:\n",
    "        idx = rng.choice(len(E_obs), size=MAX_TRAIN_HYPEREDGES, replace=False)\n",
    "        E_obs_train = [E_obs[i] for i in idx]\n",
    "    else:\n",
    "        E_obs_train = E_obs\n",
    "\n",
    "    train_sizes = [len(S) for S in E_obs_train]\n",
    "    neg_train = random_negative_hyperedges(G.nodes(), train_sizes, existing_set, rng)\n",
    "    y = np.array([1] * len(E_obs_train) + [0] * len(neg_train))\n",
    "    X = pd.DataFrame([hyperlink_features(G_obs, E_obs_train, S) for S in (E_obs_train + neg_train)])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=trial, stratify=y)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=300, random_state=trial, n_jobs=-1)\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    fit_time = time.time() - t0\n",
    "\n",
    "    test_sizes = [len(S) for S in E_miss]\n",
    "    neg_test = random_negative_hyperedges(G.nodes(), test_sizes, existing_set, rng)\n",
    "    X_test = pd.DataFrame([hyperlink_features(G_obs, E_obs_train, S) for S in (E_miss + neg_test)])\n",
    "    y_test = np.array([1] * len(E_miss) + [0] * len(neg_test))\n",
    "\n",
    "    p = clf.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, p) if len(np.unique(y_test)) > 1 else 0.5\n",
    "    return float(auc), float(fit_time)\n",
    "\n",
    "sizes = [200, 500, 1000, 2000]\n",
    "sim_rows = []\n",
    "for n in sizes:\n",
    "    for t in range(3):\n",
    "        Gs = make_sbm(n, seed=100 + t)\n",
    "        auc, ft = cheshire_fit_time_and_auc(Gs, missing_rate=0.3, trial=t)\n",
    "        sim_rows.append({\"n\": n, \"trial\": t, \"ROC-AUC\": auc, \"fit_time_s\": ft})\n",
    "        print(\"SBM n=\", n, \"trial=\", t, \"AUC=\", round(auc,3), \"fit_time=\", round(ft,2), \"s\")\n",
    "\n",
    "sim = pd.DataFrame(sim_rows)\n",
    "sim_summary = sim.groupby(\"n\").agg({\"ROC-AUC\":[\"mean\",\"std\"], \"fit_time_s\":[\"mean\",\"std\"]})\n",
    "sim_summary.columns = [\"_\".join(c) for c in sim_summary.columns]\n",
    "sim_summary = sim_summary.reset_index()\n",
    "\n",
    "# plots\n",
    "plt.figure()\n",
    "plt.plot(sim_summary[\"n\"], sim_summary[\"ROC-AUC_mean\"], marker=\"o\")\n",
    "plt.xlabel(\"n (nodes)\")\n",
    "plt.ylabel(\"CHESHIRE ROC-AUC (mean)\")\n",
    "plt.title(\"Scaling: CHESHIRE AUC vs n (SBM, ρ=0.3)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"simulation_auc_vs_n.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sim_summary[\"n\"], sim_summary[\"fit_time_s_mean\"], marker=\"o\")\n",
    "plt.xlabel(\"n (nodes)\")\n",
    "plt.ylabel(\"CHESHIRE fit time (s, mean)\")\n",
    "plt.title(\"Scaling: CHESHIRE fit time vs n (SBM, ρ=0.3)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"simulation_cheshire_fit_time.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# scaling table\n",
    "tab = sim_summary.copy()\n",
    "tab[\"ROC-AUC_mean\"] = tab[\"ROC-AUC_mean\"].map(lambda x: f\"{x:.3f}\")\n",
    "tab[\"ROC-AUC_std\"] = tab[\"ROC-AUC_std\"].map(lambda x: f\"{x:.3f}\")\n",
    "tab[\"fit_time_s_mean\"] = tab[\"fit_time_s_mean\"].map(lambda x: f\"{x:.2f}\")\n",
    "tab[\"fit_time_s_std\"] = tab[\"fit_time_s_std\"].map(lambda x: f\"{x:.2f}\")\n",
    "tab = tab.rename(columns={\n",
    "    \"n\": \"nodes\",\n",
    "    \"ROC-AUC_mean\": \"ROC-AUC (mean)\",\n",
    "    \"ROC-AUC_std\": \"ROC-AUC (sd)\",\n",
    "    \"fit_time_s_mean\": \"fit time s (mean)\",\n",
    "    \"fit_time_s_std\": \"fit time s (sd)\",\n",
    "})\n",
    "write_table(TAB_DIR / \"scaling_table.tex\",\n",
    "            latex_table(tab, \"Synthetic scaling results (SBM, $\\\\rho=0.3$): CHESHIRE ROC--AUC and training time.\", \"tab:scaling\"))\n",
    "\n",
    "print(\"Saved scaling figures + table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c65ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epjds_lp_venv",
   "language": "python",
   "name": "epjds_lp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
